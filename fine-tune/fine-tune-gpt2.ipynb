{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine tune GPT-2 for CommonGen Tasks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the GPT-2 model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_gen (C:/Users/Jingqian/.cache/huggingface/datasets/common_gen/default/2020.5.30/1a9e8bdc026c41ce7a9e96260debf7d2809cb7fd63fa02b017e4fac1b00c6b23)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccdc3a992ea249d7b5dcd3ac5c4a48ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", )\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "datasets = load_dataset('common_gen')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/68 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8da23a63744842f89a75cea0d38a0b75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ecf2e2b3e5c49979b1c3548931356b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e399008f36342559fca689b3bd0fcae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/68 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b946055cb7a64f81876aecbe74544ed8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cae1bc99970848c181788fa4cbceccfb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d12ce5093ee24567a0e397e253c37d34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_data(examples):\n",
    "    concepts = examples[\"concepts\"]\n",
    "    targets = examples[\"target\"]\n",
    "    full_string = [\" \".join(concepts[i]) + \"=\" + targets[i]  + tokenizer.eos_token\n",
    "                   for i in\n",
    "                   range(len(concepts))]\n",
    "    tokenized = tokenizer(full_string)\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "encoded_datasets = datasets.map(process_data, batched=True, remove_columns=['concept_set_idx', 'concepts', 'target'])\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    block_size = 256\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_datasets = encoded_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "' numbered spaces.<|endoftext|>work student teacher=students working with a teacher<|endoftext|>work student teacher=teacher of the deaf works with a hearing impaired student<|endoftext|>work student teacher=teacher and student working together during the training.<|endoftext|>take sky airplane=airplane takes off in the sky<|endoftext|>take sky airplane=An airplane is taking off into the sky. <|endoftext|>take sky airplane=An airplane takes off under a cloudy sky.<|endoftext|>skateboard grind rail=A man grinds a rail on his skateboard.<|endoftext|>skateboard grind rail=The teenage boy is grinding a skateboard on a steep rail.<|endoftext|>skateboard grind rail=A skateboarder in black, brown, and white is grinding his skateboard along a low rail<|endoftext|>furniture room look=The furniture in the large living room looks new.<|endoftext|>furniture room look=this piece of furniture could look good in any room.<|endoftext|>furniture room look=An antique living room with elegant looking furniture.<|endoftext|>travel forest train=A train traveling along tracks in a forest.<|endoftext|>travel forest train=Cargo train is traveling on a track next to a forest. <|endoftext|>travel forest train=A train traveling past a forest filled with green trees.<|endoftext|>gate plane park='"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][56][\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jingqian\\PycharmProjects\\huggingface-playground\\fine-tune\\GPT2-finetuned-commongen is already a clone of https://huggingface.co/liujqian/GPT2-finetuned-commongen. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "C:\\Users\\Jingqian\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4592\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1722\n",
      "  Number of trainable parameters = 124439808\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='1722' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/1722 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_17564\\1945927925.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[0meval_dataset\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlm_datasets\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"validation\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m )\n\u001B[1;32m---> 17\u001B[1;33m \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1541\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_inner_training_loop\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_train_batch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_find_batch_size\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1542\u001B[0m         )\n\u001B[1;32m-> 1543\u001B[1;33m         return inner_training_loop(\n\u001B[0m\u001B[0;32m   1544\u001B[0m             \u001B[0margs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1545\u001B[0m             \u001B[0mresume_from_checkpoint\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mresume_from_checkpoint\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36m_inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1861\u001B[0m                         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlr_scheduler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1862\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1863\u001B[1;33m                     \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1864\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mglobal_step\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1865\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepoch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0msteps_in_epoch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mzero_grad\u001B[1;34m(self, set_to_none)\u001B[0m\n\u001B[0;32m   1978\u001B[0m                     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1979\u001B[0m                         \u001B[0mp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequires_grad_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1980\u001B[1;33m                     \u001B[0mp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1981\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1982\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mshare_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mT\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mT\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"GPT2-finetuned-commongen\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    hub_token=\"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\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
